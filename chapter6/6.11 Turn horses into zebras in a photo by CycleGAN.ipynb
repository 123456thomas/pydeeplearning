{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 安装kears-crontrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://www.github.com/keras-team/keras-contrib.git\n",
      "  Cloning https://www.github.com/keras-team/keras-contrib.git to /private/var/folders/46/b7dzk4mn6g54qzptv608w7d00000gn/T/pip-t1_7py2z-build\n",
      "Requirement already satisfied: keras in /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages (from keras-contrib==2.0.8)\n",
      "Requirement already satisfied: pyyaml in /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages (from keras->keras-contrib==2.0.8)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages (from keras->keras-contrib==2.0.8)\n",
      "Requirement already satisfied: scipy>=0.14 in /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages (from keras->keras-contrib==2.0.8)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages (from keras->keras-contrib==2.0.8)\n",
      "Installing collected packages: keras-contrib\n",
      "  Running setup.py install for keras-contrib ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed keras-contrib-2.0.8\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install git+https://www.github.com/keras-team/keras-contrib.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import numpy as np\n",
    "\n",
    "class DataLoader():\n",
    "    def __init__(self, dataset_name, img_res=(128, 128)):\n",
    "        self.dataset_name = dataset_name\n",
    "        self.img_res = img_res\n",
    "\n",
    "    def load_data(self, domain, batch_size=1, is_testing=False):\n",
    "        data_type = \"train%s\" % domain if not is_testing else \"test%s\" % domain\n",
    "        path = glob('./datasets/%s/%s/*' % (self.dataset_name, data_type))\n",
    "        batch_images = np.random.choice(path, size=batch_size)\n",
    "\n",
    "        imgs = []\n",
    "        for img_path in batch_images:\n",
    "            img = self.imread(img_path)\n",
    "            if not is_testing:\n",
    "                img = scipy.misc.imresize(img, self.img_res)\n",
    "\n",
    "                if np.random.random() > 0.5:\n",
    "                    img = np.fliplr(img)\n",
    "            else:\n",
    "                img = scipy.misc.imresize(img, self.img_res)\n",
    "            imgs.append(img)\n",
    "\n",
    "        imgs = np.array(imgs)/127.5 - 1.\n",
    "\n",
    "        return imgs\n",
    "\n",
    "    def load_batch(self, batch_size=1, is_testing=False):\n",
    "        data_type = \"train\" if not is_testing else \"val\"\n",
    "        path_A = glob('./datasets/%s/%sA/*' % (self.dataset_name, data_type))\n",
    "        path_B = glob('./datasets/%s/%sB/*' % (self.dataset_name, data_type))\n",
    "        self.n_batches = int(min(len(path_A), len(path_B)) / batch_size)\n",
    "        total_samples = self.n_batches * batch_size\n",
    "\n",
    "        path_A = np.random.choice(path_A, total_samples, replace=False)\n",
    "        path_B = np.random.choice(path_B, total_samples, replace=False)\n",
    "\n",
    "        for i in range(self.n_batches-1):\n",
    "            batch_A = path_A[i*batch_size:(i+1)*batch_size]\n",
    "            batch_B = path_B[i*batch_size:(i+1)*batch_size]\n",
    "            imgs_A, imgs_B = [], []\n",
    "            for img_A, img_B in zip(batch_A, batch_B):\n",
    "                img_A = self.imread(img_A)\n",
    "                img_B = self.imread(img_B)\n",
    "\n",
    "                img_A = scipy.misc.imresize(img_A, self.img_res)\n",
    "                img_B = scipy.misc.imresize(img_B, self.img_res)\n",
    "\n",
    "                if not is_testing and np.random.random() > 0.5:\n",
    "                        img_A = np.fliplr(img_A)\n",
    "                        img_B = np.fliplr(img_B)\n",
    "\n",
    "                imgs_A.append(img_A)\n",
    "                imgs_B.append(img_B)\n",
    "\n",
    "            imgs_A = np.array(imgs_A)/127.5 - 1.\n",
    "            imgs_B = np.array(imgs_B)/127.5 - 1.\n",
    "\n",
    "            yield imgs_A, imgs_B\n",
    "\n",
    "    def load_img(self, path):\n",
    "        img = self.imread(path)\n",
    "        img = scipy.misc.imresize(img, self.img_res)\n",
    "        img = img/127.5 - 1.\n",
    "        return img[np.newaxis, :, :, :]\n",
    "\n",
    "    def imread(self, path):\n",
    "        return scipy.misc.imread(path, mode='RGB').astype(np.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 引用相关套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras_contrib.layers.normalization import InstanceNormalization\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立生成器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用U-NET网路结构\n",
    "def build_generator():\n",
    "\n",
    "    # 下采样（Down Sampling）\n",
    "    def conv2d(layer_input, filters, f_size=4):\n",
    "        d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "        d = LeakyReLU(alpha=0.2)(d)\n",
    "        d = InstanceNormalization()(d)\n",
    "        return d\n",
    "    \n",
    "    # 上采样（Upsampling）\n",
    "    def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n",
    "        u = UpSampling2D(size=2)(layer_input)\n",
    "        u = Conv2D(filters, kernel_size=f_size, strides=1, padding='same', activation='relu')(u)\n",
    "        if dropout_rate:\n",
    "            u = Dropout(dropout_rate)(u)\n",
    "        u = InstanceNormalization()(u)\n",
    "        u = Concatenate()([u, skip_input])\n",
    "        return u\n",
    "\n",
    "    #  输入图片\n",
    "    d0 = Input(shape=img_shape)\n",
    "\n",
    "    # 下采样\n",
    "    d1 = conv2d(d0, gf)\n",
    "    d2 = conv2d(d1, gf*2)\n",
    "    d3 = conv2d(d2, gf*4)\n",
    "    d4 = conv2d(d3, gf*8)\n",
    "\n",
    "    # 上采样和跨层连结\n",
    "    u1 = deconv2d(d4, d3, gf*4)\n",
    "    u2 = deconv2d(u1, d2, gf*2)\n",
    "    u3 = deconv2d(u2, d1, gf)\n",
    "\n",
    "    u4 = UpSampling2D(size=2)(u3)\n",
    "    output_img = Conv2D(channels, kernel_size=4, strides=1, padding='same', activation='tanh')(u4)\n",
    "\n",
    "    return Model(d0, output_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立鉴别器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator():\n",
    "\n",
    "    # 建立鉴别器网路层\n",
    "    def d_layer(layer_input, filters, f_size=4, normalization=True):\n",
    "        d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "        d = LeakyReLU(alpha=0.2)(d)\n",
    "        if normalization:\n",
    "            d = InstanceNormalization()(d)\n",
    "        return d\n",
    "\n",
    "    img = Input(shape=img_shape)\n",
    "\n",
    "    d1 = d_layer(img, df, normalization=False)\n",
    "    d2 = d_layer(d1, df*2)\n",
    "    d3 = d_layer(d2, df*4)\n",
    "    d4 = d_layer(d3, df*8)\n",
    "\n",
    "    validity = Conv2D(1, kernel_size=4, strides=1, padding='same')(d4)\n",
    "\n",
    "    return Model(img, validity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, batch_size=1, sample_interval=50):\n",
    "\n",
    "    start_time = datetime.datetime.now()\n",
    "\n",
    "    # 真的图片的答案判定为1生成图片判定为0\n",
    "    valid = np.ones((batch_size,) + disc_patch)\n",
    "    fake = np.zeros((batch_size,) + disc_patch)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch_i, (imgs_A, imgs_B) in enumerate(data_loader.load_batch(batch_size)):\n",
    "\n",
    "            #  训练鉴别器\n",
    "            \n",
    "            # 将图片从A翻译到B再从B翻译到A\n",
    "            fake_B = g_AB.predict(imgs_A)\n",
    "            fake_A = g_BA.predict(imgs_B)\n",
    "\n",
    "            # 训练A鉴别器\n",
    "            dA_loss_real = d_A.train_on_batch(imgs_A, valid)\n",
    "            dA_loss_fake = d_A.train_on_batch(fake_A, fake)\n",
    "            dA_loss = 0.5 * np.add(dA_loss_real, dA_loss_fake)\n",
    "\n",
    "            # 训练B鉴别器\n",
    "            dB_loss_real = d_B.train_on_batch(imgs_B, valid)\n",
    "            dB_loss_fake = d_B.train_on_batch(fake_B, fake)\n",
    "            dB_loss = 0.5 * np.add(dB_loss_real, dB_loss_fake)\n",
    "\n",
    "            # 总鉴别器损失\n",
    "            d_loss = 0.5 * np.add(dA_loss, dB_loss)\n",
    "\n",
    "\n",
    "            #  训练生成器\n",
    "            g_loss = combined.train_on_batch([imgs_A, imgs_B],\n",
    "                                                    [valid, valid,\n",
    "                                                    imgs_A, imgs_B,\n",
    "                                                    imgs_A, imgs_B])\n",
    "\n",
    "            # 计算训练时间\n",
    "            elapsed_time = datetime.datetime.now() - start_time\n",
    "\n",
    "            # 打印进度\n",
    "            print (\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f, acc: %3d%%] [G loss: %05f, adv: %05f, recon: %05f, id: %05f] time: %s \" \\\n",
    "                                                                    % ( epoch, epochs,\n",
    "                                                                        batch_i, data_loader.n_batches,\n",
    "                                                                        d_loss[0], 100*d_loss[1],\n",
    "                                                                        g_loss[0],\n",
    "                                                                        np.mean(g_loss[1:3]),\n",
    "                                                                        np.mean(g_loss[3:5]),\n",
    "                                                                        np.mean(g_loss[5:6]),\n",
    "                                                                        elapsed_time))\n",
    "\n",
    "            # 每五回合采样图片\n",
    "            if batch_i % sample_interval == 0:\n",
    "                sample_images(epoch, batch_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 图片取样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_images(epoch, batch_i):\n",
    "    os.makedirs('images/%s' % dataset_name, exist_ok=True)\n",
    "    r, c = 2, 3\n",
    "\n",
    "    imgs_A = data_loader.load_data(domain=\"A\", batch_size=1, is_testing=True)\n",
    "    imgs_B = data_loader.load_data(domain=\"B\", batch_size=1, is_testing=True)\n",
    "\n",
    "    # 将图片从A转译到B\n",
    "    fake_B = g_AB.predict(imgs_A)\n",
    "    fake_A = g_BA.predict(imgs_B)\n",
    "    # 重建图片（从B转译回A）\n",
    "    reconstr_A = g_BA.predict(fake_B)\n",
    "    reconstr_B = g_AB.predict(fake_A)\n",
    "\n",
    "    # 生成图片\n",
    "    gen_imgs = np.concatenate([imgs_A, fake_B, reconstr_A, imgs_B, fake_A, reconstr_B])\n",
    "\n",
    "    # 将图片标准化到0  -  1之间的范围\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "    titles = ['Original', 'Translated', 'Reconstructed']\n",
    "    fig, axs = plt.subplots(r, c)\n",
    "    cnt = 0\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            axs[i,j].imshow(gen_imgs[cnt])\n",
    "            axs[i, j].set_title(titles[j])\n",
    "            axs[i,j].axis('off')\n",
    "            cnt += 1\n",
    "    fig.savefig(\"images/%s/%d_%d.png\" % (dataset_name, epoch, batch_i))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建构模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 图片参数\n",
    "img_rows = 128\n",
    "img_cols = 128\n",
    "channels = 3\n",
    "img_shape = (img_rows, img_cols, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "dataset_name = 'horse2zebra'\n",
    "data_loader = DataLoader(dataset_name=dataset_name, img_res=(img_rows, img_cols))\n",
    "\n",
    "\n",
    "# 计算D输出的维度(PatchGAN)\n",
    "patch = int(img_rows / 2**4)\n",
    "disc_patch = (patch, patch, 1)\n",
    "\n",
    "# 生成器与鉴别器第一层的神经元数\n",
    "gf = 32\n",
    "df = 64\n",
    "\n",
    "# 损失\n",
    "lambda_cycle = 10.0               # 循环一致性损失\n",
    "lambda_id = 0.1 * lambda_cycle    # Identity loss\n",
    "\n",
    "optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "# 建立以及编译鉴别器\n",
    "d_A = build_discriminator()\n",
    "d_B = build_discriminator()\n",
    "d_A.compile(loss='mse',\n",
    "    optimizer=optimizer,\n",
    "    metrics=['accuracy'])\n",
    "d_B.compile(loss='mse',\n",
    "    optimizer=optimizer,\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立生成器\n",
    "g_AB = build_generator()\n",
    "g_BA = build_generator()\n",
    "\n",
    "# 从A B两个Domain输入图片\n",
    "img_A = Input(shape=img_shape)\n",
    "img_B = Input(shape=img_shape)\n",
    "\n",
    "# 从Domain A 将图片转译到 Domain B\n",
    "fake_B = g_AB(img_A)\n",
    "fake_A = g_BA(img_B)\n",
    "# 从Domain B 将图片转译回 Domain A (重建)\n",
    "reconstr_A = g_BA(fake_B)\n",
    "reconstr_B = g_AB(fake_A)\n",
    "\n",
    "# 建立图片识别ID\n",
    "img_A_id = g_BA(img_A)\n",
    "img_B_id = g_AB(img_B)\n",
    "\n",
    "# 只训练生成器\n",
    "d_A.trainable = False\n",
    "d_B.trainable = False\n",
    "\n",
    "# 利用鉴别器辨别图片真伪\n",
    "valid_A = d_A(fake_A)\n",
    "valid_B = d_B(fake_B)\n",
    "\n",
    "# 训练生成器骗过鉴别器\n",
    "combined = Model(inputs=[img_A, img_B],\n",
    "                      outputs=[ valid_A, valid_B,\n",
    "                                reconstr_A, reconstr_B,\n",
    "                                img_A_id, img_B_id ])\n",
    "combined.compile(loss=['mse', 'mse',\n",
    "                       'mae', 'mae',\n",
    "                       'mae', 'mae'],\n",
    "                    loss_weights=[  1, 1,\n",
    "                                    lambda_cycle, lambda_cycle,\n",
    "                                    lambda_id, lambda_id ],\n",
    "                    optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练CycleGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/ipykernel_launcher.py:70: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/ipykernel_launcher.py:48: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "/Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/ipykernel_launcher.py:49: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "/Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/keras/engine/training.py:973: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/20] [Batch 0/106] [D loss: 28.575975, acc:  12%] [G loss: 32.662563, adv: 8.985338, recon: 0.674166, id: 0.594438] time: 0:00:31.423852 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/ipykernel_launcher.py:23: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/20] [Batch 1/106] [D loss: 18.708851, acc:  10%] [G loss: 23.850031, adv: 3.718084, recon: 0.754779, id: 0.677637] time: 0:00:44.625135 \n",
      "[Epoch 0/20] [Batch 2/106] [D loss: 6.108274, acc:  18%] [G loss: 33.770657, adv: 8.953201, recon: 0.727703, id: 0.710256] time: 0:00:56.064625 \n",
      "[Epoch 0/20] [Batch 3/106] [D loss: 3.475364, acc:  28%] [G loss: 29.591640, adv: 6.659639, recon: 0.748904, id: 0.690904] time: 0:01:07.432699 \n",
      "[Epoch 0/20] [Batch 4/106] [D loss: 3.221425, acc:  20%] [G loss: 17.634134, adv: 0.862468, recon: 0.734473, id: 0.689285] time: 0:01:19.437452 \n",
      "[Epoch 0/20] [Batch 5/106] [D loss: 0.714048, acc:  47%] [G loss: 17.721844, adv: 2.011209, recon: 0.629395, id: 0.547526] time: 0:01:31.504293 \n",
      "[Epoch 0/20] [Batch 6/106] [D loss: 0.645739, acc:  43%] [G loss: 13.123373, adv: 0.862360, recon: 0.519493, id: 0.462641] time: 0:01:43.504518 \n",
      "[Epoch 0/20] [Batch 7/106] [D loss: 0.787574, acc:  43%] [G loss: 13.718512, adv: 1.350994, recon: 0.503539, id: 0.429093] time: 0:01:54.740666 \n",
      "[Epoch 0/20] [Batch 8/106] [D loss: 0.970325, acc:  28%] [G loss: 13.659564, adv: 1.041140, recon: 0.527135, id: 0.496040] time: 0:02:06.080894 \n",
      "[Epoch 0/20] [Batch 9/106] [D loss: 0.471840, acc:  49%] [G loss: 11.915409, adv: 0.888824, recon: 0.461483, id: 0.424693] time: 0:02:17.697654 \n",
      "[Epoch 0/20] [Batch 10/106] [D loss: 0.762572, acc:  32%] [G loss: 12.306341, adv: 0.850705, recon: 0.484055, id: 0.421063] time: 0:02:30.591739 \n",
      "[Epoch 0/20] [Batch 11/106] [D loss: 0.491958, acc:  47%] [G loss: 11.105484, adv: 0.734089, recon: 0.440020, id: 0.365465] time: 0:02:42.662326 \n",
      "[Epoch 0/20] [Batch 12/106] [D loss: 0.842938, acc:  37%] [G loss: 13.983242, adv: 2.164642, recon: 0.438804, id: 0.396814] time: 0:02:55.212664 \n",
      "[Epoch 0/20] [Batch 13/106] [D loss: 0.972428, acc:  29%] [G loss: 11.505424, adv: 1.171039, recon: 0.418842, id: 0.335155] time: 0:03:06.266501 \n",
      "[Epoch 0/20] [Batch 14/106] [D loss: 0.628826, acc:  37%] [G loss: 11.140991, adv: 0.914501, recon: 0.425709, id: 0.358431] time: 0:03:18.550905 \n",
      "[Epoch 0/20] [Batch 15/106] [D loss: 0.501068, acc:  40%] [G loss: 9.925622, adv: 0.801843, recon: 0.379278, id: 0.310269] time: 0:03:29.999167 \n",
      "[Epoch 0/20] [Batch 16/106] [D loss: 0.470744, acc:  43%] [G loss: 9.659277, adv: 0.797871, recon: 0.366169, id: 0.334668] time: 0:03:42.405834 \n",
      "[Epoch 0/20] [Batch 17/106] [D loss: 0.414259, acc:  41%] [G loss: 9.477484, adv: 0.765683, recon: 0.361051, id: 0.343578] time: 0:03:53.981685 \n",
      "[Epoch 0/20] [Batch 18/106] [D loss: 0.498410, acc:  36%] [G loss: 9.828744, adv: 0.910583, recon: 0.362643, id: 0.341482] time: 0:04:05.641736 \n",
      "[Epoch 0/20] [Batch 19/106] [D loss: 0.483392, acc:  36%] [G loss: 8.741083, adv: 0.814658, recon: 0.322846, id: 0.297498] time: 0:04:17.366413 \n",
      "[Epoch 0/20] [Batch 20/106] [D loss: 0.370838, acc:  47%] [G loss: 9.263303, adv: 0.854210, recon: 0.344884, id: 0.303205] time: 0:04:28.819025 \n",
      "[Epoch 0/20] [Batch 21/106] [D loss: 0.635375, acc:  26%] [G loss: 8.881618, adv: 0.815170, recon: 0.329634, id: 0.278310] time: 0:04:40.837503 \n",
      "[Epoch 0/20] [Batch 22/106] [D loss: 0.569491, acc:  30%] [G loss: 9.135737, adv: 0.999781, recon: 0.325257, id: 0.274390] time: 0:04:53.098643 \n",
      "[Epoch 0/20] [Batch 23/106] [D loss: 0.584641, acc:  34%] [G loss: 8.669146, adv: 0.812454, recon: 0.322299, id: 0.300882] time: 0:05:05.653186 \n",
      "[Epoch 0/20] [Batch 24/106] [D loss: 0.562605, acc:  46%] [G loss: 9.372819, adv: 1.161517, recon: 0.321195, id: 0.300572] time: 0:05:16.990541 \n",
      "[Epoch 0/20] [Batch 25/106] [D loss: 0.620655, acc:  36%] [G loss: 9.233192, adv: 0.881532, recon: 0.340201, id: 0.290958] time: 0:05:29.194180 \n",
      "[Epoch 0/20] [Batch 26/106] [D loss: 0.877993, acc:  24%] [G loss: 8.940521, adv: 1.173755, recon: 0.300148, id: 0.257892] time: 0:05:42.438219 \n",
      "[Epoch 0/20] [Batch 27/106] [D loss: 0.682866, acc:  29%] [G loss: 9.351605, adv: 0.822612, recon: 0.351265, id: 0.305338] time: 0:05:53.910125 \n",
      "[Epoch 0/20] [Batch 28/106] [D loss: 0.480340, acc:  44%] [G loss: 8.262336, adv: 0.744805, recon: 0.308894, id: 0.260915] time: 0:06:05.861886 \n",
      "[Epoch 0/20] [Batch 29/106] [D loss: 0.766631, acc:  22%] [G loss: 9.018046, adv: 0.978350, recon: 0.322547, id: 0.295291] time: 0:06:18.261049 \n",
      "[Epoch 0/20] [Batch 30/106] [D loss: 0.410141, acc:  49%] [G loss: 7.349681, adv: 0.766972, recon: 0.264295, id: 0.226925] time: 0:06:30.548612 \n",
      "[Epoch 0/20] [Batch 31/106] [D loss: 0.585200, acc:  39%] [G loss: 8.436836, adv: 0.915928, recon: 0.300806, id: 0.267883] time: 0:06:42.639041 \n",
      "[Epoch 0/20] [Batch 32/106] [D loss: 0.482830, acc:  35%] [G loss: 7.627066, adv: 0.625977, recon: 0.289475, id: 0.285009] time: 0:06:58.737720 \n",
      "[Epoch 0/20] [Batch 33/106] [D loss: 0.479496, acc:  28%] [G loss: 7.564777, adv: 0.820549, recon: 0.268061, id: 0.277997] time: 0:07:12.513315 \n",
      "[Epoch 0/20] [Batch 34/106] [D loss: 0.332750, acc:  58%] [G loss: 6.575590, adv: 0.745172, recon: 0.229645, id: 0.250545] time: 0:07:25.154446 \n",
      "[Epoch 0/20] [Batch 35/106] [D loss: 0.491654, acc:  33%] [G loss: 6.681424, adv: 0.618106, recon: 0.247230, id: 0.222624] time: 0:07:37.067888 \n",
      "[Epoch 0/20] [Batch 36/106] [D loss: 0.298093, acc:  58%] [G loss: 7.159369, adv: 0.782056, recon: 0.254650, id: 0.230598] time: 0:07:49.122689 \n",
      "[Epoch 0/20] [Batch 37/106] [D loss: 0.284771, acc:  58%] [G loss: 7.199167, adv: 0.730371, recon: 0.260904, id: 0.237645] time: 0:08:02.368531 \n",
      "[Epoch 0/20] [Batch 38/106] [D loss: 0.356427, acc:  49%] [G loss: 7.630574, adv: 0.837498, recon: 0.271411, id: 0.234002] time: 0:08:16.898366 \n",
      "[Epoch 0/20] [Batch 39/106] [D loss: 0.332034, acc:  51%] [G loss: 7.290131, adv: 0.720631, recon: 0.266351, id: 0.216736] time: 0:08:31.743371 \n",
      "[Epoch 0/20] [Batch 40/106] [D loss: 0.369441, acc:  47%] [G loss: 7.157687, adv: 0.717778, recon: 0.259114, id: 0.252753] time: 0:08:45.143801 \n",
      "[Epoch 0/20] [Batch 41/106] [D loss: 0.456104, acc:  30%] [G loss: 7.271990, adv: 0.672203, recon: 0.270129, id: 0.260308] time: 0:08:57.252001 \n",
      "[Epoch 0/20] [Batch 42/106] [D loss: 0.421531, acc:  39%] [G loss: 7.577761, adv: 0.749154, recon: 0.276096, id: 0.238649] time: 0:09:10.438974 \n",
      "[Epoch 0/20] [Batch 43/106] [D loss: 0.396340, acc:  38%] [G loss: 6.648175, adv: 0.707993, recon: 0.237663, id: 0.207917] time: 0:09:23.097391 \n",
      "[Epoch 0/20] [Batch 44/106] [D loss: 0.403028, acc:  47%] [G loss: 6.926346, adv: 0.670185, recon: 0.253928, id: 0.269494] time: 0:09:36.288678 \n",
      "[Epoch 0/20] [Batch 45/106] [D loss: 0.451373, acc:  39%] [G loss: 7.313987, adv: 0.891717, recon: 0.251147, id: 0.243813] time: 0:09:49.053093 \n",
      "[Epoch 0/20] [Batch 46/106] [D loss: 0.376204, acc:  51%] [G loss: 7.688402, adv: 0.943798, recon: 0.265733, id: 0.236583] time: 0:10:01.990249 \n",
      "[Epoch 0/20] [Batch 47/106] [D loss: 0.557356, acc:  31%] [G loss: 6.897057, adv: 0.656006, recon: 0.254539, id: 0.217121] time: 0:10:15.369143 \n",
      "[Epoch 0/20] [Batch 48/106] [D loss: 0.495997, acc:  33%] [G loss: 6.645420, adv: 0.674187, recon: 0.241508, id: 0.210930] time: 0:10:28.974393 \n",
      "[Epoch 0/20] [Batch 49/106] [D loss: 0.337804, acc:  53%] [G loss: 7.384045, adv: 0.715422, recon: 0.271308, id: 0.254058] time: 0:10:41.879710 \n",
      "[Epoch 0/20] [Batch 50/106] [D loss: 0.429698, acc:  36%] [G loss: 6.871365, adv: 0.592005, recon: 0.258959, id: 0.235529] time: 0:10:55.817856 \n",
      "[Epoch 0/20] [Batch 51/106] [D loss: 0.378795, acc:  49%] [G loss: 6.937396, adv: 0.664847, recon: 0.255744, id: 0.229188] time: 0:11:09.128576 \n",
      "[Epoch 0/20] [Batch 52/106] [D loss: 0.304589, acc:  57%] [G loss: 7.078065, adv: 0.704860, recon: 0.259081, id: 0.231886] time: 0:11:22.552447 \n",
      "[Epoch 0/20] [Batch 53/106] [D loss: 0.429980, acc:  36%] [G loss: 6.618457, adv: 0.612848, recon: 0.244671, id: 0.244128] time: 0:11:35.298005 \n",
      "[Epoch 0/20] [Batch 54/106] [D loss: 0.332643, acc:  49%] [G loss: 6.413017, adv: 0.673810, recon: 0.229095, id: 0.223417] time: 0:11:48.483889 \n",
      "[Epoch 0/20] [Batch 55/106] [D loss: 0.360003, acc:  47%] [G loss: 6.583321, adv: 0.650118, recon: 0.240718, id: 0.198374] time: 0:12:02.335953 \n",
      "[Epoch 0/20] [Batch 56/106] [D loss: 0.510749, acc:  27%] [G loss: 6.187535, adv: 0.703663, recon: 0.217488, id: 0.200401] time: 0:12:15.005000 \n",
      "[Epoch 0/20] [Batch 57/106] [D loss: 0.406794, acc:  37%] [G loss: 6.320154, adv: 0.648032, recon: 0.228361, id: 0.208230] time: 0:12:27.330822 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/20] [Batch 58/106] [D loss: 0.288422, acc:  62%] [G loss: 5.999347, adv: 0.651062, recon: 0.212798, id: 0.208702] time: 0:12:39.768991 \n",
      "[Epoch 0/20] [Batch 59/106] [D loss: 0.333338, acc:  48%] [G loss: 6.156004, adv: 0.655350, recon: 0.220536, id: 0.185324] time: 0:12:52.239997 \n",
      "[Epoch 0/20] [Batch 60/106] [D loss: 0.418815, acc:  33%] [G loss: 6.048318, adv: 0.691613, recon: 0.212553, id: 0.184398] time: 0:13:04.458802 \n",
      "[Epoch 0/20] [Batch 61/106] [D loss: 0.417786, acc:  32%] [G loss: 6.052221, adv: 0.567244, recon: 0.224231, id: 0.180085] time: 0:13:17.053322 \n",
      "[Epoch 0/20] [Batch 62/106] [D loss: 0.481978, acc:  35%] [G loss: 8.068579, adv: 0.856917, recon: 0.291965, id: 0.255859] time: 0:13:29.361450 \n",
      "[Epoch 0/20] [Batch 63/106] [D loss: 0.505195, acc:  32%] [G loss: 6.862192, adv: 0.672155, recon: 0.252961, id: 0.210994] time: 0:13:42.409581 \n",
      "[Epoch 0/20] [Batch 64/106] [D loss: 0.275887, acc:  55%] [G loss: 6.230003, adv: 0.707281, recon: 0.219813, id: 0.202858] time: 0:13:54.694557 \n",
      "[Epoch 0/20] [Batch 65/106] [D loss: 0.461212, acc:  37%] [G loss: 6.482645, adv: 0.613882, recon: 0.239981, id: 0.225177] time: 0:14:07.532461 \n",
      "[Epoch 0/20] [Batch 66/106] [D loss: 0.404773, acc:  44%] [G loss: 6.241936, adv: 0.664496, recon: 0.224122, id: 0.180314] time: 0:14:19.967055 \n",
      "[Epoch 0/20] [Batch 67/106] [D loss: 0.556645, acc:  32%] [G loss: 6.838672, adv: 0.871298, recon: 0.232418, id: 0.231527] time: 0:14:33.871954 \n",
      "[Epoch 0/20] [Batch 68/106] [D loss: 0.406100, acc:  44%] [G loss: 5.926651, adv: 0.785766, recon: 0.198753, id: 0.161790] time: 0:14:47.927497 \n",
      "[Epoch 0/20] [Batch 69/106] [D loss: 0.476953, acc:  36%] [G loss: 6.567411, adv: 0.575393, recon: 0.248093, id: 0.207222] time: 0:15:01.094892 \n",
      "[Epoch 0/20] [Batch 70/106] [D loss: 0.447135, acc:  37%] [G loss: 6.247237, adv: 0.560795, recon: 0.233274, id: 0.198174] time: 0:15:13.821351 \n",
      "[Epoch 0/20] [Batch 71/106] [D loss: 0.291117, acc:  51%] [G loss: 6.583356, adv: 0.713762, recon: 0.235216, id: 0.209552] time: 0:15:26.524852 \n",
      "[Epoch 0/20] [Batch 72/106] [D loss: 0.352168, acc:  49%] [G loss: 6.403930, adv: 0.590812, recon: 0.238326, id: 0.220738] time: 0:15:38.917472 \n",
      "[Epoch 0/20] [Batch 73/106] [D loss: 0.397465, acc:  44%] [G loss: 6.112198, adv: 0.634090, recon: 0.219603, id: 0.220010] time: 0:15:51.339795 \n",
      "[Epoch 0/20] [Batch 74/106] [D loss: 0.437001, acc:  36%] [G loss: 6.571079, adv: 0.587673, recon: 0.245579, id: 0.200752] time: 0:16:03.774514 \n",
      "[Epoch 0/20] [Batch 75/106] [D loss: 0.386724, acc:  41%] [G loss: 6.133666, adv: 0.592063, recon: 0.225113, id: 0.209039] time: 0:16:16.182480 \n",
      "[Epoch 0/20] [Batch 76/106] [D loss: 0.292030, acc:  60%] [G loss: 6.080709, adv: 0.660402, recon: 0.217130, id: 0.199166] time: 0:16:28.591858 \n",
      "[Epoch 0/20] [Batch 77/106] [D loss: 0.411732, acc:  39%] [G loss: 5.262732, adv: 0.516467, recon: 0.191537, id: 0.185029] time: 0:16:40.956599 \n",
      "[Epoch 0/20] [Batch 78/106] [D loss: 0.334857, acc:  46%] [G loss: 5.965025, adv: 0.716328, recon: 0.206953, id: 0.182958] time: 0:16:53.916435 \n",
      "[Epoch 0/20] [Batch 79/106] [D loss: 0.351950, acc:  51%] [G loss: 6.068573, adv: 0.628672, recon: 0.218902, id: 0.206614] time: 0:17:06.176767 \n",
      "[Epoch 0/20] [Batch 80/106] [D loss: 0.353075, acc:  50%] [G loss: 6.019786, adv: 0.596297, recon: 0.219884, id: 0.204573] time: 0:17:19.472816 \n",
      "[Epoch 0/20] [Batch 81/106] [D loss: 0.422442, acc:  32%] [G loss: 5.995832, adv: 0.604226, recon: 0.218968, id: 0.163856] time: 0:17:31.731229 \n",
      "[Epoch 0/20] [Batch 82/106] [D loss: 0.393619, acc:  38%] [G loss: 6.497218, adv: 0.721321, recon: 0.230587, id: 0.226503] time: 0:17:44.257887 \n",
      "[Epoch 0/20] [Batch 83/106] [D loss: 0.469570, acc:  37%] [G loss: 5.842926, adv: 0.544655, recon: 0.216483, id: 0.201265] time: 0:17:56.250007 \n",
      "[Epoch 0/20] [Batch 84/106] [D loss: 0.348929, acc:  40%] [G loss: 5.706005, adv: 0.647354, recon: 0.200123, id: 0.201667] time: 0:18:08.632474 \n",
      "[Epoch 0/20] [Batch 85/106] [D loss: 0.434468, acc:  41%] [G loss: 6.101789, adv: 0.644827, recon: 0.219709, id: 0.188287] time: 0:18:21.894509 \n",
      "[Epoch 0/20] [Batch 86/106] [D loss: 0.410119, acc:  38%] [G loss: 6.548683, adv: 0.742721, recon: 0.231245, id: 0.215531] time: 0:18:34.596065 \n",
      "[Epoch 0/20] [Batch 87/106] [D loss: 0.336300, acc:  53%] [G loss: 5.987791, adv: 0.592645, recon: 0.218944, id: 0.181729] time: 0:18:47.073612 \n",
      "[Epoch 0/20] [Batch 88/106] [D loss: 0.362919, acc:  38%] [G loss: 5.673891, adv: 0.605953, recon: 0.202270, id: 0.199919] time: 0:18:59.637947 \n",
      "[Epoch 0/20] [Batch 89/106] [D loss: 0.365377, acc:  40%] [G loss: 5.558128, adv: 0.575254, recon: 0.201029, id: 0.175092] time: 0:19:13.211731 \n",
      "[Epoch 0/20] [Batch 90/106] [D loss: 0.394199, acc:  31%] [G loss: 5.488563, adv: 0.545961, recon: 0.200327, id: 0.192873] time: 0:19:25.324429 \n",
      "[Epoch 0/20] [Batch 91/106] [D loss: 0.361740, acc:  46%] [G loss: 6.028574, adv: 0.769881, recon: 0.204507, id: 0.167953] time: 0:19:38.686225 \n",
      "[Epoch 0/20] [Batch 92/106] [D loss: 0.292210, acc:  60%] [G loss: 5.362494, adv: 0.638190, recon: 0.185615, id: 0.161069] time: 0:19:53.331135 \n",
      "[Epoch 0/20] [Batch 93/106] [D loss: 0.399938, acc:  36%] [G loss: 5.981604, adv: 0.546077, recon: 0.222927, id: 0.214052] time: 0:20:09.320247 \n",
      "[Epoch 0/20] [Batch 94/106] [D loss: 0.317364, acc:  50%] [G loss: 5.389006, adv: 0.667392, recon: 0.184743, id: 0.178311] time: 0:20:22.865124 \n",
      "[Epoch 0/20] [Batch 95/106] [D loss: 0.497387, acc:  33%] [G loss: 5.699423, adv: 0.709167, recon: 0.194551, id: 0.176027] time: 0:20:36.404432 \n",
      "[Epoch 0/20] [Batch 96/106] [D loss: 0.343080, acc:  49%] [G loss: 5.464434, adv: 0.653748, recon: 0.188839, id: 0.152772] time: 0:20:50.707735 \n",
      "[Epoch 0/20] [Batch 97/106] [D loss: 0.345151, acc:  50%] [G loss: 5.171330, adv: 0.514706, recon: 0.188494, id: 0.159099] time: 0:21:04.707203 \n",
      "[Epoch 0/20] [Batch 98/106] [D loss: 0.436608, acc:  38%] [G loss: 6.421741, adv: 0.610867, recon: 0.237614, id: 0.220835] time: 0:21:17.260102 \n",
      "[Epoch 0/20] [Batch 99/106] [D loss: 0.311540, acc:  54%] [G loss: 5.492571, adv: 0.700399, recon: 0.185811, id: 0.136976] time: 0:21:29.140216 \n",
      "[Epoch 0/20] [Batch 100/106] [D loss: 0.283968, acc:  57%] [G loss: 5.565076, adv: 0.537783, recon: 0.204868, id: 0.159680] time: 0:21:41.448566 \n",
      "[Epoch 0/20] [Batch 101/106] [D loss: 0.341140, acc:  45%] [G loss: 5.915519, adv: 0.671434, recon: 0.207898, id: 0.210030] time: 0:21:53.473465 \n"
     ]
    }
   ],
   "source": [
    "train(epochs=20, batch_size=10, sample_interval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
