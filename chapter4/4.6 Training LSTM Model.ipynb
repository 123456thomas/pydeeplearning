{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取IMDB文字数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "607140"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import pandas\n",
    "\n",
    "df = pandas.read_csv('https://github.com/ywchiu/pydeeplearning/raw/master/data/movie_review.csv', index_col=0)\n",
    "#df.head()\n",
    "reviews = ''.join([' <SOR> '+ele+' <EOR> ' for ele in df['content'].head(300).tolist()]).lower()\n",
    "len(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 产生训练用序列与目标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练用序列长度\n",
    "maxlen = 60\n",
    "\n",
    "# 随机采样序列\n",
    "step = 3\n",
    "\n",
    "sentences = []\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(reviews) - maxlen, step):\n",
    "    # 根据步骤，每次取出maxlen的序列做为特征\n",
    "    sentences.append(reviews[i: i + maxlen])\n",
    "    # 取出maxlen + 1的字当目标\n",
    "    next_chars.append(reviews[i + maxlen])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202360"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## one-hot 编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 找出独特字元\n",
    "chars = sorted(list(set(reviews)))\n",
    "len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 利用字典映射char和索引\n",
    "char_indices = dict((char, chars.index(char)) for char in chars)\n",
    "#char_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x：训练样本\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "\n",
    "# y：目标样本\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "\n",
    "for i, sentence in enumerate(sentences):        \n",
    "    for t, char in enumerate(sentence):         \n",
    "        x[i, t, char_indices[char]] = 1         \n",
    "    y[i, char_indices[next_chars[i]]] = 1       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(202360, 60, 57)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(202360, 57)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立LSTM模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "# 输入为60 *字串长度\n",
    "model.add(layers.LSTM(128,input_shape=(maxlen, len(chars))))\n",
    "\n",
    "# 以字元对应作为输出\n",
    "model.add(layers.Dense(len(chars), activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint('imdb2.hdf5', monitor='loss', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "202240/202360 [============================>.] - ETA: 0s - loss: 1.6878Epoch 00001: loss improved from inf to 1.68773, saving model to imdb2.hdf5\n",
      "202360/202360 [==============================] - 125s 618us/step - loss: 1.6877\n",
      "Epoch 2/15\n",
      "202240/202360 [============================>.] - ETA: 0s - loss: 1.4176Epoch 00002: loss improved from 1.68773 to 1.41755, saving model to imdb2.hdf5\n",
      "202360/202360 [==============================] - 135s 670us/step - loss: 1.4175\n",
      "Epoch 3/15\n",
      "202240/202360 [============================>.] - ETA: 0s - loss: 1.1391Epoch 00003: loss improved from 1.41755 to 1.13904, saving model to imdb2.hdf5\n",
      "202360/202360 [==============================] - 141s 696us/step - loss: 1.1390\n",
      "Epoch 4/15\n",
      "202240/202360 [============================>.] - ETA: 0s - loss: 0.8919Epoch 00004: loss improved from 1.13904 to 0.89180, saving model to imdb2.hdf5\n",
      "202360/202360 [==============================] - 145s 717us/step - loss: 0.8918\n",
      "Epoch 5/15\n",
      "202240/202360 [============================>.] - ETA: 0s - loss: 0.6942Epoch 00005: loss improved from 0.89180 to 0.69411, saving model to imdb2.hdf5\n",
      "202360/202360 [==============================] - 144s 711us/step - loss: 0.6941\n",
      "Epoch 6/15\n",
      "202240/202360 [============================>.] - ETA: 0s - loss: 0.5484Epoch 00006: loss improved from 0.69411 to 0.54839, saving model to imdb2.hdf5\n",
      "202360/202360 [==============================] - 144s 713us/step - loss: 0.5484\n",
      "Epoch 7/15\n",
      "202240/202360 [============================>.] - ETA: 0s - loss: 0.4444Epoch 00007: loss improved from 0.54839 to 0.44435, saving model to imdb2.hdf5\n",
      "202360/202360 [==============================] - 145s 717us/step - loss: 0.4444\n",
      "Epoch 8/15\n",
      "202240/202360 [============================>.] - ETA: 0s - loss: 0.3626Epoch 00008: loss improved from 0.44435 to 0.36256, saving model to imdb2.hdf5\n",
      "202360/202360 [==============================] - 145s 717us/step - loss: 0.3626\n",
      "Epoch 9/15\n",
      "202240/202360 [============================>.] - ETA: 0s - loss: 0.3021Epoch 00009: loss improved from 0.36256 to 0.30207, saving model to imdb2.hdf5\n",
      "202360/202360 [==============================] - 145s 719us/step - loss: 0.3021\n",
      "Epoch 10/15\n",
      "202240/202360 [============================>.] - ETA: 0s - loss: 0.2551Epoch 00010: loss improved from 0.30207 to 0.25506, saving model to imdb2.hdf5\n",
      "202360/202360 [==============================] - 146s 721us/step - loss: 0.2551\n",
      "Epoch 11/15\n",
      "202240/202360 [============================>.] - ETA: 0s - loss: 0.2260Epoch 00011: loss improved from 0.25506 to 0.22604, saving model to imdb2.hdf5\n",
      "202360/202360 [==============================] - 146s 723us/step - loss: 0.2260\n",
      "Epoch 12/15\n",
      "202240/202360 [============================>.] - ETA: 0s - loss: 0.1978Epoch 00012: loss improved from 0.22604 to 0.19785, saving model to imdb2.hdf5\n",
      "202360/202360 [==============================] - 146s 724us/step - loss: 0.1979\n",
      "Epoch 13/15\n",
      "202240/202360 [============================>.] - ETA: 0s - loss: 0.1830Epoch 00013: loss improved from 0.19785 to 0.18296, saving model to imdb2.hdf5\n",
      "202360/202360 [==============================] - 147s 724us/step - loss: 0.1830\n",
      "Epoch 14/15\n",
      "202240/202360 [============================>.] - ETA: 0s - loss: 0.1522Epoch 00014: loss improved from 0.18296 to 0.15230, saving model to imdb2.hdf5\n",
      "202360/202360 [==============================] - 147s 726us/step - loss: 0.1523\n",
      "Epoch 15/15\n",
      "202240/202360 [============================>.] - ETA: 0s - loss: 0.1542Epoch 00015: loss did not improve\n",
      "202360/202360 [==============================] - 147s 726us/step - loss: 0.1542\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x, y, batch_size=128, epochs=15, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4-Copy1.6 Training LSTM Model.ipynb     4.6 Training LSTM Model.ipynb\r\n",
      "4.4 Stock Prediction.ipynb              imdb.hdf5\r\n",
      "4.5 Retrieve IMDB User's Reviews.ipynb  imdb2.hdf5\r\n"
     ]
    }
   ],
   "source": [
    "%ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rest assured, iron man is an absolutely amazing movie. i won'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[7:67]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
